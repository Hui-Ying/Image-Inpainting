{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inpainting- Partial Convolution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhxml5lgl0hi",
        "colab_type": "text"
      },
      "source": [
        "# **1. Before Running**\n",
        "\n",
        "**Before running any code**, make sure to install all the .zip files provided in the following links and upload the .zip files to Colab. These files are needed for the training and testing code in train.py and testing.py. There are currently four data sets that are being used right now: cropped training images, cropped validation images, and their respective masks. Note that the training images may take a bit longer to upload.\n",
        "\n",
        "- [cropped_train_img.zip](https://drive.google.com/open?id=1175apAFzI-1g11XkRwqD3jVaQzUwXTyf)\n",
        "\n",
        "- [cropped_valid_img.zip](https://drive.google.com/open?id=1kkRmPTaiw9ejnhf_vT14r-vQUHZPNe5U)\n",
        "\n",
        "\n",
        "- [mask_train_img.zip](https://drive.google.com/open?id=1cN3GAX7xM5f20ZT3pKIRAVWm6Ho9CjCT)\n",
        "\n",
        "- [mask_valid_img.zip](https://drive.google.com/open?id=14Z7TqCE28tkVVmR76BUUozk_QVPLr6W0)\n",
        "\n",
        "**Note**: Newest masks are in the folder in the link below:\n",
        "\n",
        "- [Models_and_Masks_and_Images_Folder](https://drive.google.com/drive/folders/1b4JQlOfVfJhmETd8JzUXS42l5xWO7M_D?usp=sharing)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# **2. Original Data Set Links and Explanation**\n",
        "\n",
        "The original data sets required for our final project can be found here: [ICME19 Inpainting Challenge](https://icme19inpainting.github.io/) or if the links on that page don't work, then look here: [Inpainting Challenge Data Sets on BitaHub](https://forum.bitahub.com/views/activity-detail-en.html?activityId=_2e9eb0c6eba94190beb6430941ff4c13://).\n",
        "\n",
        "The training and validation sets of these original data sets have the following names:\n",
        "- valid_img\n",
        "- train_img\n",
        "\n",
        "The valid_img data set that you download actually contains **two types of validation images**: one for **error concealment (EC)** and the other is for **object removal (OR)**. More information is on the ICME19 Inpainting Challenge link above.\n",
        "\n",
        "# **3. Our Current Data Sets For The Final Project**\n",
        "The code we have based our work off of, bobqywei's GitHub code, works only on 256 x 256 images. However, the dataset provided for our final project are not 256 x 256. So, we have cropped all the images and the cropped images are stored in the folders titled:\n",
        "\n",
        "- cropped_train_img\n",
        "- cropped_valid_img\n",
        "\n",
        "\n",
        "However, **the data on the ICME webpage only provides ground truth images**. As a result, we need to generate images with holes ourselves. This is where the **mask data sets** come in. The mask data sets that we have created generate 256x256 sized masks to match the cropping size. There are mask data sets both for the validation and training ground truth images:\n",
        "\n",
        "- mask_train_img\n",
        "- mask_valid_img\n",
        "\n",
        "# **4. How to Run the Code on this Colab**\n",
        "\n",
        "1. **Download** the mask and cropped data sets from the links mentioned above.\n",
        "2. **Unzip** all the data sets in the folders into Colab using the code below. This may take some time as well.\n",
        "3. **Pip Install** all necessary packages using the code below.\n",
        "4. **Compile** all coding blocks **in order**. (i.e., first run places2_train.py, then run partial_conv_net.py, then loss.py, etc.) This is because there is a dependency between files. For example, `train.py` and `test.py` rely on `loss.py` functions, so `loss.py` functions must be compiled first.\n",
        "5. (Optional) **Test** `inpaint.py`. If you guys already have a Python environment set up on your own personal computers, then it shouldn't be hard to test the `inpaint.py` yourselves if you want. The code is commented out at the end of the Colab. The `inpaint.py` code cannot run on the Colab, since it needs to run on a physical machine. To run this, you need to download one of the models in the `model` folder. Depending on which one, you may have to change the name of the model that is hardcoded in `inpaint.py`.\n",
        "\n",
        "**Note:** If we compile the functions in the correct order, we don't need to have import statements (ex: `from places2_train import Places2Data`). `import` is only necessary if all the code is separated into separate files. However, since we copied all functions from all the files into a single Colab Notebook, **`import` isn't needed if you compile everything in the correct order.**\n",
        "\n",
        "**Note2:** This means you guys shouldn't have to download all the files from bobqywei's GitHub. Instead, just make a copy of this Colab, make your own modifications, then copy the modifications back into this Notebook once you think code is working correctly.\n",
        "\n",
        "\n",
        "\n",
        "# **5. Things We Need To Do**\n",
        "1. Fix our model so that `inpaint.py` and `test.py` generate good results.\n",
        "2. Create a larger data set using data augmentation as mentioned in class. If possible, this data set needs to be larger than 5000 images. Currently, we have been provided only 1500.\n",
        "3. Make `test.py` output more quantifiable performance metrics.\n",
        "4. Read more on *Partial Convolutions* and start writing the paper as we go along.\n",
        "5. Explore different network models besides VGG16\n",
        "6. Double check that the partial_conv_net is correctly implemented and follows the paper.\n",
        "\n",
        "**Note:** `test.py` actually generates an output .jpg file showing the results of the inpainting called `test.jpg`. You can see how good/bad the inpainting is based on that image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDAHZwQ9Zfjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "74a8e656-f6ae-47ee-e719-42865f37bfa5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZYCB2gHt4V7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "881f9bce-9dd3-4ca7-c9da-ffd46cf9cf73"
      },
      "source": [
        "# unzips all data sets into Google Colab\n",
        "!unzip cropped_train_img.zip\n",
        "!unzip cropped_valid_img.zip\n",
        "!unzip mask_train_img.zip\n",
        "!unzip mask_valid_img.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open cropped_train_img.zip, cropped_train_img.zip.zip or cropped_train_img.zip.ZIP.\n",
            "unzip:  cannot find or open cropped_valid_img.zip, cropped_valid_img.zip.zip or cropped_valid_img.zip.ZIP.\n",
            "unzip:  cannot find or open mask_train_img.zip, mask_train_img.zip.zip or mask_train_img.zip.ZIP.\n",
            "unzip:  cannot find or open mask_valid_img.zip, mask_valid_img.zip.zip or mask_valid_img.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgY1Z3VzuJPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68a69e48-aed5-4dfa-d6fa-803883f5cb2e"
      },
      "source": [
        "# pip installation\n",
        "!pip install tensorboardcolab\n",
        "# !pip install PyQt5==5.9.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXfJWRwmHDax",
        "colab_type": "text"
      },
      "source": [
        "#**places2_train.py**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zglPDSUhHBPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision import utils\n",
        "\n",
        "\n",
        "# mean and std channel values for places2 dataset\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STDDEV = [0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "# reverses the earlier normalization applied to the image to prepare output\n",
        "def unnormalize(x):\n",
        "\tx.transpose_(1, 3)\n",
        "\tx = x * torch.Tensor(STDDEV) + torch.Tensor(MEAN)\n",
        "\tx.transpose_(1, 3)\n",
        "\treturn x\n",
        "\n",
        "\n",
        "class Places2Data(torch.utils.data.Dataset):\n",
        "\n",
        "\tdef __init__(self, path_to_data, path_to_mask):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.img_paths = glob.glob(os.path.dirname(os.path.abspath('')) + path_to_data + \"/**/*.jpg\", recursive=True)\n",
        "\t\tself.mask_paths = glob.glob(os.path.dirname(os.path.abspath('')) + path_to_mask + \"/*.png\")\n",
        "\t\tself.num_masks = len(self.mask_paths)\n",
        "\t\tself.num_imgs = len(self.img_paths)\n",
        "\t\tself.img_transform1 = transforms.Compose([transforms.Resize((256,256)),\n",
        "  transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "\ttransforms.RandomVerticalFlip(),\n",
        "\ttransforms.RandomCrop(size=None, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.RandomRotation(20)]) \n",
        "\t\t# normalizes the image: (img - MEAN) / STD and converts to tensor\n",
        "\t\tself.img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STDDEV)])\n",
        "\t\tself.mask_transform = transforms.ToTensor()\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.num_imgs\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tgt_img = Image.open(self.img_paths[index])\n",
        "\t\n",
        "\t\tgt_img = self.img_transform(gt_img.convert('RGB'))\n",
        "\n",
        "\t\tmask = Image.open(self.mask_paths[random.randint(0, self.num_masks - 1)])\n",
        "\t\tmask = self.mask_transform(mask.convert('RGB'))\n",
        "\n",
        "\t\treturn gt_img * mask, mask, gt_img\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMZwZderGuFO",
        "colab_type": "text"
      },
      "source": [
        "#**partial_conv_net.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP4uw0JJGaVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PartialConvLayer (nn.Module):\n",
        "\n",
        "\tdef __init__(self, in_channels, out_channels, bn=True, bias=False, sample=\"none-3\", activation=\"relu\"):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.bn = bn\n",
        "\n",
        "\t\tif sample == \"down-7\":\n",
        "\t\t\t# Kernel Size = 7, Stride = 2, Padding = 3\n",
        "\t\t\tself.input_conv = nn.Conv2d(in_channels, out_channels, 7, 2, 3, bias=bias)\n",
        "\t\t\tself.mask_conv = nn.Conv2d(in_channels, out_channels, 7, 2, 3, bias=False)\n",
        "\n",
        "\t\telif sample == \"down-5\":\n",
        "\t\t\tself.input_conv = nn.Conv2d(in_channels, out_channels, 5, 2, 2, bias=bias)\n",
        "\t\t\tself.mask_conv = nn.Conv2d(in_channels, out_channels, 5, 2, 2, bias=False)\n",
        "\n",
        "\t\telif sample == \"down-3\":\n",
        "\t\t\tself.input_conv = nn.Conv2d(in_channels, out_channels, 3, 2, 1, bias=bias)\n",
        "\t\t\tself.mask_conv = nn.Conv2d(in_channels, out_channels, 3, 2, 1, bias=False)\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tself.input_conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=bias)\n",
        "\t\t\tself.mask_conv = nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False)\n",
        "\n",
        "\t\tnn.init.constant_(self.mask_conv.weight, 1.0)\n",
        "\n",
        "\t\t# \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\"\n",
        "\t\t# negative slope of leaky_relu set to 0, same as relu\n",
        "\t\t# \"fan_in\" preserved variance from forward pass\n",
        "\t\tnn.init.kaiming_normal_(self.input_conv.weight, a=0, mode=\"fan_in\")\n",
        "\n",
        "\t\tfor param in self.mask_conv.parameters():\n",
        "\t\t\tparam.requires_grad = False\n",
        "\n",
        "\t\tif bn:\n",
        "\t\t\t# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
        "\t\t\t# Applying BatchNorm2d layer after Conv will remove the channel mean\n",
        "\t\t\tself.batch_normalization = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "\t\tif activation == \"relu\":\n",
        "\t\t\t# Used between all encoding layers\n",
        "\t\t\tself.activation = nn.ReLU()\n",
        "\t\telif activation == \"leaky_relu\":\n",
        "\t\t\t# Used between all decoding layers (Leaky RELU with alpha = 0.2)\n",
        "\t\t\tself.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "\tdef forward(self, input_x, mask):\n",
        "\t\t# output = W^T dot (X .* M) + b\n",
        "\t\toutput = self.input_conv(input_x * mask)\n",
        "\n",
        "\t\t# requires_grad = False\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# mask = (1 dot M) + 0 = M\n",
        "\t\t\toutput_mask = self.mask_conv(mask)\n",
        "\n",
        "\t\tif self.input_conv.bias is not None:\n",
        "\t\t\t# spreads existing bias values out along 2nd dimension (channels) and then expands to output size\n",
        "\t\t\toutput_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(output)\n",
        "\t\telse:\n",
        "\t\t\toutput_bias = torch.zeros_like(output)\n",
        "\n",
        "\t\t# mask_sum is the sum of the binary mask at every partial convolution location\n",
        "\t\tmask_is_zero = (output_mask == 0)\n",
        "\t\t# temporarily sets zero values to one to ease output calculation \n",
        "\t\tmask_sum = output_mask.masked_fill_(mask_is_zero, 1.0)\n",
        "\n",
        "\t\t# output at each location as follows:\n",
        "\t\t# output = (W^T dot (X .* M) + b - b) / M_sum + b ; if M_sum > 0\n",
        "\t\t# output = 0 ; if M_sum == 0\n",
        "\t\toutput = (output - output_bias) / mask_sum + output_bias\n",
        "\t\toutput = output.masked_fill_(mask_is_zero, 0.0)\n",
        "\n",
        "\t\t# mask is updated at each location\n",
        "\t\tnew_mask = torch.ones_like(output)\n",
        "\t\tnew_mask = new_mask.masked_fill_(mask_is_zero, 0.0)\n",
        "\n",
        "\t\tif self.bn:\n",
        "\t\t\toutput = self.batch_normalization(output)\n",
        "\n",
        "\t\tif hasattr(self, 'activation'):\n",
        "\t\t\toutput = self.activation(output)\n",
        "\n",
        "\t\treturn output, new_mask\n",
        "\n",
        "\n",
        "class PartialConvUNet(nn.Module):\n",
        "\n",
        "\t# 256 x 256 image input, 256 = 2^8\n",
        "\tdef __init__(self, input_size=256, layers=7):\n",
        "\t\tif 2 ** (layers + 1) != input_size:\n",
        "\t\t\traise AssertionError\n",
        "\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.freeze_enc_bn = False\n",
        "\t\tself.layers = layers\n",
        "\n",
        "\t\t# ======================= ENCODING LAYERS =======================\n",
        "\t\t# 3x256x256 --> 64x128x128\n",
        "\t\tself.encoder_1 = PartialConvLayer(3, 64, bn=False, sample=\"down-7\")\n",
        "\n",
        "\t\t# 64x128x128 --> 128x64x64\n",
        "\t\tself.encoder_2 = PartialConvLayer(64, 128, sample=\"down-5\")\n",
        "\n",
        "\t\t# 128x64x64 --> 256x32x32\n",
        "\t\tself.encoder_3 = PartialConvLayer(128, 256, sample=\"down-3\")\n",
        "\n",
        "\t\t# 256x32x32 --> 512x16x16\n",
        "\t\tself.encoder_4 = PartialConvLayer(256, 512, sample=\"down-3\")\n",
        "\n",
        "\t\t# 512x16x16 --> 512x8x8 --> 512x4x4 --> 512x2x2\n",
        "\t\tfor i in range(5, layers + 1):\n",
        "\t\t\tname = \"encoder_{:d}\".format(i)\n",
        "\t\t\tsetattr(self, name, PartialConvLayer(512, 512, sample=\"down-3\"))\n",
        "\n",
        "\t\t# ======================= DECODING LAYERS =======================\n",
        "\t\t# dec_7: UP(512x2x2) + 512x4x4(enc_6 output) = 1024x4x4 --> 512x4x4\n",
        "\t\t# dec_6: UP(512x4x4) + 512x8x8(enc_5 output) = 1024x8x8 --> 512x8x8\n",
        "\t\t# dec_5: UP(512x8x8) + 512x16x16(enc_4 output) = 1024x16x16 --> 512x16x16\n",
        "\t\tfor i in range(5, layers + 1):\n",
        "\t\t\tname = \"decoder_{:d}\".format(i)\n",
        "\t\t\tsetattr(self, name, PartialConvLayer(512 + 512, 512, activation=\"leaky_relu\"))\n",
        "\n",
        "\t\t# UP(512x16x16) + 256x32x32(enc_3 output) = 768x32x32 --> 256x32x32\n",
        "\t\tself.decoder_4 = PartialConvLayer(512 + 256, 256, activation=\"leaky_relu\")\n",
        "\n",
        "\t\t# UP(256x32x32) + 128x64x64(enc_2 output) = 384x64x64 --> 128x64x64\n",
        "\t\tself.decoder_3 = PartialConvLayer(256 + 128, 128, activation=\"leaky_relu\")\n",
        "\n",
        "\t\t# UP(128x64x64) + 64x128x128(enc_1 output) = 192x128x128 --> 64x128x128\n",
        "\t\tself.decoder_2 = PartialConvLayer(128 + 64, 64, activation=\"leaky_relu\")\n",
        "\n",
        "\t\t# UP(64x128x128) + 3x256x256(original image) = 67x256x256 --> 3x256x256(final output)\n",
        "\t\tself.decoder_1 = PartialConvLayer(64 + 3, 3, bn=False, activation=\"\", bias=True)\n",
        "\t\n",
        "\tdef forward(self, input_x, mask):\n",
        "\t\tencoder_dict = {}\n",
        "\t\tmask_dict = {}\n",
        "\n",
        "\t\tkey_prev = \"h_0\"\n",
        "\t\tencoder_dict[key_prev], mask_dict[key_prev] = input_x, mask\n",
        "\n",
        "\t\tfor i in range(1, self.layers + 1):\n",
        "\t\t\tencoder_key = \"encoder_{:d}\".format(i)\n",
        "\t\t\tkey = \"h_{:d}\".format(i)\n",
        "\t\t\t# Passes input and mask through encoding layer\n",
        "\t\t\tencoder_dict[key], mask_dict[key] = getattr(self, encoder_key)(encoder_dict[key_prev], mask_dict[key_prev])\n",
        "\t\t\tkey_prev = key\n",
        "\n",
        "\t\t# Gets the final output data and mask from the encoding layers\n",
        "\t\t# 512 x 2 x 2\n",
        "\t\tout_key = \"h_{:d}\".format(self.layers)\n",
        "\t\tout_data, out_mask = encoder_dict[out_key], mask_dict[out_key]\n",
        "\n",
        "\t\tfor i in range(self.layers, 0, -1):\n",
        "\t\t\tencoder_key = \"h_{:d}\".format(i - 1)\n",
        "\t\t\tdecoder_key = \"decoder_{:d}\".format(i)\n",
        "\n",
        "\t\t\t# Upsample to 2 times scale, matching dimensions of previous encoding layer output\n",
        "\t\t\tout_data = F.interpolate(out_data, scale_factor=2)\n",
        "\t\t\tout_mask = F.interpolate(out_mask, scale_factor=2)\n",
        "\n",
        "\t\t\t# concatenate upsampled decoder output with encoder output of same H x W dimensions\n",
        "\t\t\t# s.t. final decoding layer input will contain the original image\n",
        "\t\t\tout_data = torch.cat([out_data, encoder_dict[encoder_key]], dim=1)\n",
        "\t\t\t# also concatenate the masks\n",
        "\t\t\tout_mask = torch.cat([out_mask, mask_dict[encoder_key]], dim=1)\n",
        "\t\t\t\n",
        "\t\t\t# feed through decoder layers\n",
        "\t\t\tout_data, out_mask = getattr(self, decoder_key)(out_data, out_mask)\n",
        "\n",
        "\t\treturn out_data\n",
        "\n",
        "\tdef train(self, mode=True):\n",
        "\t\tsuper().train(mode)\n",
        "\t\tif self.freeze_enc_bn:\n",
        "\t\t\tfor name, module in self.named_modules():\n",
        "\t\t\t\tif isinstance(module, nn.BatchNorm2d) and \"enc\" in name:\n",
        "\t\t\t\t\t# Sets batch normalization layers to evaluation mode\n",
        "\t\t\t\t\tmodule.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRGtvruaFzhP",
        "colab_type": "text"
      },
      "source": [
        "# **Loss.py** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q322r9WuFaw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "import pdb\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# from places2_train import Places2Data, MEAN, STDDEV\n",
        "\n",
        "LAMBDAS = {\"valid\": 1.0, \"hole\": 6.0, \"tv\": 2.0, \"perceptual\": 0.05, \"style\": 240.0}\n",
        "\n",
        "\n",
        "def gram_matrix(feature_matrix):\n",
        "\t(batch, channel, h, w) = feature_matrix.size()\n",
        "\tfeature_matrix = feature_matrix.view(batch, channel, h * w)\n",
        "\tfeature_matrix_t = feature_matrix.transpose(1, 2)\n",
        "\n",
        "\t# batch matrix multiplication * normalization factor K_n\n",
        "\t# (batch, channel, h * w) x (batch, h * w, channel) ==> (batch, channel, channel)\n",
        "\tgram = torch.bmm(feature_matrix, feature_matrix_t) / (channel * h * w)\n",
        "\n",
        "\t# size = (batch, channel, channel)\n",
        "\treturn gram\n",
        "\n",
        "\n",
        "def perceptual_loss(h_comp, h_out, h_gt, l1):\n",
        "\tloss = 0.0\n",
        "\n",
        "\tfor i in range(len(h_comp)):\n",
        "\t\tloss += l1(h_out[i], h_gt[i])\n",
        "\t\tloss += l1(h_comp[i], h_gt[i])\n",
        "\n",
        "\treturn loss\n",
        "\n",
        "\n",
        "def style_loss(h_comp, h_out, h_gt, l1):\n",
        "\tloss = 0.0\n",
        "\n",
        "\tfor i in range(len(h_comp)):\n",
        "\t\tloss += l1(gram_matrix(h_out[i]), gram_matrix(h_gt[i]))\n",
        "\t\tloss += l1(gram_matrix(h_comp[i]), gram_matrix(h_gt[i]))\n",
        "\n",
        "\treturn loss\n",
        "\n",
        "\n",
        "# computes TV loss over entire composed image since gradient will not be passed backward to input\n",
        "def total_variation_loss(image, l1):\n",
        "    # shift one pixel and get loss1 difference (for both x and y direction)\n",
        "    loss = l1(image[:, :, :, :-1], image[:, :, :, 1:]) + l1(image[:, :, :-1, :], image[:, :, 1:, :])\n",
        "    return loss\n",
        "\n",
        "\n",
        "class VGG16Extractor(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tvgg16 = models.vgg16(pretrained=True)\n",
        "\t\tself.max_pooling1 = vgg16.features[:5]\n",
        "\t\tself.max_pooling2 = vgg16.features[5:10]\n",
        "\t\tself.max_pooling3 = vgg16.features[10:17]\n",
        "\n",
        "\t\tfor i in range(1, 4):\n",
        "\t\t\tfor param in getattr(self, 'max_pooling{:d}'.format(i)).parameters():\n",
        "\t\t\t\tparam.requires_grad = False\n",
        "\n",
        "\t# feature extractor at each of the first three pooling layers\n",
        "\tdef forward(self, image):\n",
        "\t\tresults = [image]\n",
        "\t\tfor i in range(1, 4):\n",
        "\t\t\tfunc = getattr(self, 'max_pooling{:d}'.format(i))\n",
        "\t\t\tresults.append(func(results[-1]))\n",
        "\t\treturn results[1:]\n",
        "\n",
        "\n",
        "class CalculateLoss(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.vgg_extract = VGG16Extractor()\n",
        "\t\tself.l1 = nn.L1Loss()\n",
        "\n",
        "\tdef forward(self, input_x, mask, output, ground_truth):\n",
        "\t\tcomposed_output = (input_x * mask) + (output * (1 - mask))\n",
        "\n",
        "\t\tfs_composed_output = self.vgg_extract(composed_output)\n",
        "\t\tfs_output = self.vgg_extract(output)\n",
        "\t\tfs_ground_truth = self.vgg_extract(ground_truth)\n",
        "\n",
        "\t\tloss_dict = dict()\n",
        "\n",
        "\t\tloss_dict[\"hole\"] = self.l1((1 - mask) * output, (1 - mask) * ground_truth) * LAMBDAS[\"hole\"]\n",
        "\t\tloss_dict[\"valid\"] = self.l1(mask * output, mask * ground_truth) * LAMBDAS[\"valid\"]\n",
        "\t\tloss_dict[\"perceptual\"] = perceptual_loss(fs_composed_output, fs_output, fs_ground_truth, self.l1) * LAMBDAS[\"perceptual\"]\n",
        "\t\tloss_dict[\"style\"] = style_loss(fs_composed_output, fs_output, fs_ground_truth, self.l1) * LAMBDAS[\"style\"]\n",
        "\t\tloss_dict[\"tv\"] = total_variation_loss(composed_output, self.l1) * LAMBDAS[\"tv\"]\n",
        "\n",
        "\t\treturn loss_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmyeIQ_sGAqC",
        "colab_type": "text"
      },
      "source": [
        "#**Train.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqCkEqZct7Ni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import easydict\n",
        "from torch.utils import data\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pdb\n",
        "\n",
        "\n",
        "class SubsetSampler(data.sampler.Sampler):\n",
        "\tdef __init__(self, start_sample, num_samples):\n",
        "\t\tself.num_samples = num_samples\n",
        "\t\tself.start_sample = start_sample\n",
        "\n",
        "\tdef __iter__(self):\n",
        "\t\treturn iter(range(self.start_sample, self.num_samples))\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.num_samples\n",
        "\n",
        "\n",
        "def requires_grad(param):\n",
        "\treturn param.requires_grad\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  args = easydict.EasyDict({\n",
        "      \"train_path\": \"/cropped_train_img\",\n",
        "      \"mask_path\": \"/mask_train_img\",\n",
        "      \"val_path\": \"/valid_img/data_png/error_concealment_valid\",\n",
        "      \"log_dir\": \"/training_logs\",\n",
        "      \"save_dir\": \"/model\",\n",
        "      \"load_model\": \"\",\n",
        "      \"lr\": 5e-3,\n",
        "      \"fine_tune_lr\": 5e-3,\n",
        "      \"batch_size\": 16,\n",
        "      \"epochs\": 15,\n",
        "      \"fine_tune\": 0,\n",
        "      \"gpu\": 0,\n",
        "      \"num_workers\": 0,\n",
        "      \"log_interval\": 10,\n",
        "      \"save_interval\": 5000\n",
        "  })\n",
        "  cwd = os.getcwd()\n",
        "  print(f\"cwd: {cwd}\")\n",
        "  \n",
        "\n",
        "\n",
        "\t#Tensorboard SummaryWriter setup\n",
        "  if not os.path.exists(cwd + args.log_dir):\n",
        "\t\t  os.makedirs(cwd + args.log_dir)\n",
        "\n",
        "  writer = SummaryWriter(cwd + args.log_dir)\n",
        "\n",
        "  if not os.path.exists(cwd + args.save_dir):\n",
        "\t\t  os.makedirs(cwd + args.save_dir)\n",
        "\n",
        "  if args.gpu >= 0:\n",
        "      device = torch.device(\"cuda:{}\".format(args.gpu))\n",
        "  else:\n",
        "      device = torch.device(\"gpu\")\n",
        "\n",
        "  print(f\"Path to training ground truth images: {cwd+args.train_path}\")\n",
        "  print(f\"Path to training mask images: {cwd+args.mask_path}\")\n",
        "\n",
        "  data_train = Places2Data(cwd+args.train_path, cwd+args.mask_path)\n",
        "  data_size = len(data_train)\n",
        "  print(\"Loaded training dataset with {} samples and {} masks\".format(data_size, data_train.num_masks))\n",
        "\n",
        "\t# assert(data_size % args.batch_size == 0)\n",
        "  iters_per_epoch = data_size // args.batch_size\n",
        "\n",
        "\t# data_val = Places2Data(args.val_path, args.mask_path)\n",
        "\t# print(\"Loaded validation dataset...\")\n",
        "\n",
        "\t# Move model to gpu prior to creating optimizer, since parameters become different objects after loading\n",
        "  model = PartialConvUNet().to(device)\n",
        "  print(\"Loaded model to device...\")\n",
        "\n",
        "\t# Set the fine tune learning rate if necessary\n",
        "  if args.fine_tune:\n",
        "      lr = args.fine_tune_lr\n",
        "      model.freeze_enc_bn = True\n",
        "  else:\n",
        "      lr = args.lr\n",
        "\n",
        "\t# Adam optimizer proposed in: \"Adam: A Method for Stochastic Optimization\"\n",
        "\t# filters the model parameters for those with requires_grad == True\n",
        "  optimizer = torch.optim.Adam(filter(requires_grad, model.parameters()), lr=lr)\n",
        "  print(\"Setup Adam optimizer...\")\n",
        "\n",
        "\t# Loss function\n",
        "\t# Moves vgg16 model to gpu, used for feature map in loss function\n",
        "  loss_func = CalculateLoss().to(device)\n",
        "  print(\"Setup loss function...\")\n",
        "\n",
        "\t# Resume training on model\n",
        "  if args.load_model:\n",
        "      assert os.path.isfile(cwd + args.save_dir + args.load_model)\n",
        "\n",
        "      filename = cwd + args.save_dir + args.load_model\n",
        "      checkpoint_dict = torch.load(filename)\n",
        "\n",
        "      model.load_state_dict(checkpoint_dict[\"model\"])\n",
        "      optimizer.load_state_dict(checkpoint_dict[\"optimizer\"])\n",
        "\n",
        "      print(\"Resume training on model:{}\".format(args.load_model))\n",
        "\n",
        "\t\t# Load all parameters to gpu\n",
        "      model = model.to(device)\n",
        "      for state in optimizer.state.values():\n",
        "          for key, value in state.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "              state[key] = value.to(device)\n",
        "\n",
        "  for epoch in range(0, args.epochs):\n",
        "\n",
        "      iterator_train = iter(data.DataLoader(data_train, \n",
        "\t\t\t\t\t\t\t\t\t\t\tbatch_size=args.batch_size, \n",
        "\t\t\t\t\t\t\t\t\t\t\tnum_workers=args.num_workers, \n",
        "\t\t\t\t\t\t\t\t\t\t\tsampler=SubsetSampler(0, data_size)))\n",
        "\n",
        "\t\t# TRAINING LOOP\n",
        "      print(\"\\nEPOCH:{} of {} - starting training loop from iteration:0 to iteration:{}\\n\".format(epoch, args.epochs, iters_per_epoch))\n",
        "\t\t\n",
        "      for i in tqdm(range(0, iters_per_epoch)):\n",
        "\n",
        "\t\t\t# Sets model to train mode\n",
        "        model.train()\n",
        "\n",
        "        # Gets the next batch of images\n",
        "        image, mask, gt = [x.to(device) for x in next(iterator_train)]\n",
        "        \n",
        "        # Forward-propagates images through net\n",
        "        # Mask is also propagated, though it is usually gone by the decoding stage\n",
        "        output = model(image, mask)\n",
        "\n",
        "        loss_dict = loss_func(image, mask, output, gt)\n",
        "        loss = 0.0\n",
        "\n",
        "\t\t\t# sums up each loss value\n",
        "        for key, value in loss_dict.items():\n",
        "          loss += value\n",
        "          if (i + 1) % args.log_interval == 0:\n",
        "            writer.add_scalar(key, value.item(), (epoch * iters_per_epoch) + i + 1)\n",
        "            writer.file_writer.flush()\n",
        "\n",
        "        # Resets gradient accumulator in optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # back-propogates gradients through model weights\n",
        "        loss.backward()\n",
        "        # updates the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save model\n",
        "        if (i + 1) % args.save_interval == 0 or (i + 1) == iters_per_epoch:\n",
        "          filename = cwd + args.save_dir + \"/model_e{}_i{}.pth\".format(epoch, i + 1)\n",
        "          state = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
        "          torch.save(state, filename)\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYCPNj0eGUuI",
        "colab_type": "text"
      },
      "source": [
        "#**Test.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKZUC-NVGIud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms\n",
        "from skimage.measure import compare_psnr as psnr\n",
        "import skimage\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from places2_train import MEAN, STDDEV, unnormalize\n",
        "from partial_conv_net import PartialConvUNet\n",
        "from loss import CalculateLoss\n",
        "\n",
        "\n",
        "def display_image(img):\n",
        "\tplt.figure()\n",
        "\tplt.imshow(img)\n",
        "\tplt.show()\n",
        "\n",
        "image_num = str(random.randint(0, 69))\n",
        "mask_num = str(random.randint(0, 1749))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "      \"img\": \"/valid_img/{image_num}_0.jpg\",\n",
        "      \"mask\": \"/mask_valid_img/mask_{}.png\".format(image_num),\n",
        "      \"model\": \"/model/model_e99_i96.pth\",\n",
        "      \"size\": 256\n",
        "  })\n",
        "\n",
        "\n",
        "\n",
        "img_list = []\n",
        "mask_list = []\n",
        "\n",
        "loss_hole = []\n",
        "loss_valid = []\n",
        "loss_perceptual = []\n",
        "loss_style = []\n",
        "loss_tv = []\n",
        "loss_ssim = []\n",
        "loss_psnr = []\n",
        "\n",
        "# random.seed(0)\n",
        "\n",
        "samples = 100\n",
        "\n",
        "for i in range(samples):\n",
        "\n",
        "    # based on the structure of the ICME 2019 Challenge Dataset\n",
        "    folder_num = random.randint(0,1)\n",
        "    if(folder_num == 0):\n",
        "        val_type = \"error_concealment_valid\"\n",
        "    else:\n",
        "        val_type = \"object_removal_valid\"\n",
        "    image_num = str(random.randint(0, 69))\n",
        "    mask_num = image_num\n",
        "    mask_subnum = str(random.randint(0, 69))\n",
        "    img_name = f\"/valid_img/data_png/valid_img/{image_num}.jpg\"\n",
        "    mask_name = f\"/valid_img/data_png/{val_type}/{mask_num}/{mask_num}_{mask_subnum}.png\"\n",
        "\n",
        "    # # For testing custom data set\n",
        "    # image_num = str(random.randint(0, 69))\n",
        "    # image_subnum = str(random.randint(0,4))\n",
        "    # mask_num = str(random.randint(0,1749))\n",
        "    # img_name = f\"/cropped_valid_img/{image_num}_{image_subnum}.jpg\"\n",
        "    # mask_name = f\"/mask_valid_img/mask_{mask_num}.png\"\n",
        "\n",
        "    print(f\"img, mask: {img_name}, {mask_name}\")\n",
        "    img_list.append(img_name)\n",
        "    mask_list.append(mask_name)\n",
        "\n",
        "cwd = os.getcwd()\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "img_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STDDEV)])\n",
        "mask_transform = transforms.ToTensor()\n",
        "transform_resize = transforms.Resize(size=(256,256))\n",
        "\n",
        "\n",
        "# earlier epoch\n",
        "checkpoint_dict = torch.load(cwd + args.model, map_location=\"cpu\")\n",
        "model = PartialConvUNet()\n",
        "model.load_state_dict(checkpoint_dict[\"model\"])\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# later epoch\n",
        "checkpoint_dict2 = torch.load(cwd + \"/model/model_e152_i96.pth\", map_location=\"cpu\")\n",
        "model2 = PartialConvUNet()\n",
        "model2.load_state_dict(checkpoint_dict2[\"model\"])\n",
        "model2 = model2.to(device)\n",
        "model2.eval()\n",
        "\n",
        "\n",
        "for k,(img,mask) in enumerate(zip(img_list,mask_list)):\n",
        "    print(f\"img: {img}, mask = {mask}\")\n",
        "    # with Image.open(cwd + img) as gt_img, Image.open(cwd + mask) as mask:\n",
        "        \n",
        "    mask = Image.open(cwd + mask)\n",
        "    mask = transform_resize(mask)\n",
        "    mask = mask_transform(mask.convert(\"RGB\"))\n",
        "\n",
        "    gt_img = Image.open(cwd + img)\n",
        "    gt_img = transform_resize(gt_img)\n",
        "    gt_img = img_transform(gt_img.convert(\"RGB\"))\n",
        "    img = gt_img * mask\n",
        "\n",
        "    img.unsqueeze_(0)\n",
        "    gt_img.unsqueeze_(0)\n",
        "    mask.unsqueeze_(0)\n",
        "\n",
        "    \n",
        "    # for the first model\n",
        "    with torch.no_grad():\n",
        "        output = model(img.to(device), mask.to(device))\n",
        "\n",
        "    output = (mask * img) + ((1 - mask) * output)\n",
        "\n",
        "\n",
        "    # for the second model\n",
        "    with torch.no_grad():\n",
        "        output2 = model2(img.to(device), mask.to(device))\n",
        "\n",
        "    output2 = (mask * img) + ((1 - mask) * output2)\n",
        "    loss_func = CalculateLoss()\n",
        "    loss_out = loss_func(img, mask, output2, gt_img)\n",
        "\n",
        "\n",
        "    gt_img_ssim = gt_img.squeeze(0)\n",
        "    output2_ssim = output2.squeeze(0)\n",
        "\n",
        "\n",
        "    gt_img_ssim = torchvision.transforms.ToPILImage()(gt_img_ssim)\n",
        "    output2_ssim = torchvision.transforms.ToPILImage()(output2_ssim)\n",
        "    \n",
        "    \n",
        "    for i,(key, value) in enumerate(loss_out.items()):\n",
        "        # print(\"KEY:{} | VALUE:{}\".format(key, value))\n",
        "        if(key == \"hole\"):\n",
        "            loss_hole.append(value)\n",
        "        if(key == \"valid\"):\n",
        "            loss_valid.append(value)\n",
        "        if(key == \"perceptual\"):\n",
        "            loss_perceptual.append(value)\n",
        "        if(key == \"tv\"):\n",
        "            loss_tv.append(value)\n",
        "        if(key == \"style\"):\n",
        "            loss_style.append(value)\n",
        "    \n",
        "        \n",
        "        \n",
        "    gt_img_ssim = np.array(gt_img_ssim)\n",
        "    output2_ssim = np.array(output2_ssim)\n",
        "    out_ssim = ssim(gt_img_ssim,output2_ssim, multichannel=True)\n",
        "    out_psnr = psnr(gt_img_ssim,output2_ssim)\n",
        "    loss_ssim.append(out_ssim)\n",
        "    loss_psnr.append(out_psnr)\n",
        "\n",
        "\n",
        "avg_hole = sum(loss_hole)/len(loss_hole)\n",
        "avg_valid = sum(loss_valid)/len(loss_valid)\n",
        "avg_tv = sum(loss_tv)/len(loss_tv)\n",
        "avg_perceptual = sum(loss_perceptual)/len(loss_perceptual)\n",
        "avg_style = sum(loss_style)/len(loss_style)\n",
        "avg_ssim = sum(loss_ssim)/len(loss_ssim)\n",
        "avg_psnr = sum(loss_psnr)/len(loss_psnr)\n",
        "\n",
        "print(f\"Average loss: hole value - {avg_hole}\")\n",
        "print(f\"Average loss: valid value - {avg_valid}\")\n",
        "print(f\"Average loss: tv value - {avg_tv}\")\n",
        "print(f\"Average loss: perceptual value - {avg_perceptual}\")\n",
        "print(f\"Average loss: style value - {avg_style}\")\n",
        "\n",
        "print(f\"Average: PSNR value - {avg_psnr}\")\n",
        "print(f\"Average: ssim value - {avg_ssim}\")\n",
        "\n",
        "\n",
        "\n",
        "grid = make_grid(torch.cat((unnormalize(gt_img, MEAN, STDDEV), unnormalize(img, MEAN, STDDEV), unnormalize(output, MEAN, STDDEV), unnormalize(output2, MEAN, STDDEV)), dim=0))\n",
        "plt.figure()\n",
        "plt.imshow(torchvision.transforms.ToPILImage()(grid))\n",
        "save_image(grid, \"test.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}